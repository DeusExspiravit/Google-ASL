{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bda2f5ee",
   "metadata": {},
   "source": [
    "so my plan is that i would have 2 different iterations of the dataset: one iteration would be the raw dataset. The second iteration would have the resultant vector from every consecutive frame that would provide us with the vector values of the movement of the observed landmark. Now both of the datasets would be reshapes to better resemble an image even if they are far from an recognisable image. If the resizing is not feasable, we would transform the \"images\" into a uniform shape so that it could pass the model. \n",
    "\n",
    "After that is done, i would create a model with 2 convolutional branches that woudl recognize teh different characteristics of the \"images\", and whatever features it was able to extract from the \"images\" woould be utilized in the final prediction process as the two braches merge into each other(note that the vanilla version of the dataset brach would be be independant, but the other branch would be heavily influenced by the vanilla branch. This is done so that landmark movement vector has a reference point in space. \n",
    "\n",
    "The outputs of the model, both for training and validation, would also be transformed into image-like state so as to simplify the prediction process(note that since not all of the clips do not have the same frames, there woudl be a few null values, and to take care of those values we would just replace those values with 0. This is done because the data we are using are spatial data, and any misrepresentation could drastically affect the prediction of the model.\n",
    "\n",
    "Once we have the prediction, we woudld flatten out the data and then create another model, but this time it would be different, as i would be making a transfomer neural network. At the point of writting this manifesto, i have very little knowledge about transformer neural networks, so once i have gathered sufficient data regarding these model- I will be optimistic and vague."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5551a5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils import data\n",
    "import torchvision\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import smart_resize\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json,Tokenizer\n",
    "from tqdm import notebook, tqdm\n",
    "from tqdm.notebook import tnrange\n",
    "from typing import List, Tuple, Dict\n",
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import json\n",
    "import logging\n",
    "import gc\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "399fe0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available devices: mps\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Available devices: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38b3aa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/arvinprince/pytorch-files/Google ASL/data/character_to_prediction_index.json\") as json_file:\n",
    "    CHAR2ORD = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4476c1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv(\"/Users/arvinprince/pytorch-files/Google ASL/data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f0b69fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"/Users/arvinprince/pytorch-files/Google ASL/data/296317215.parquet\").astype(np.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c37cad25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 165396 entries, 1494438637 to 1526914806\n",
      "Columns: 1630 entries, frame to z_right_hand_20\n",
      "dtypes: float16(1630)\n",
      "memory usage: 515.5 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e6df145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dataset: (165396, 1630)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame</th>\n",
       "      <th>x_face_0</th>\n",
       "      <th>x_face_1</th>\n",
       "      <th>x_face_2</th>\n",
       "      <th>x_face_3</th>\n",
       "      <th>x_face_4</th>\n",
       "      <th>x_face_5</th>\n",
       "      <th>x_face_6</th>\n",
       "      <th>x_face_7</th>\n",
       "      <th>x_face_8</th>\n",
       "      <th>...</th>\n",
       "      <th>z_right_hand_11</th>\n",
       "      <th>z_right_hand_12</th>\n",
       "      <th>z_right_hand_13</th>\n",
       "      <th>z_right_hand_14</th>\n",
       "      <th>z_right_hand_15</th>\n",
       "      <th>z_right_hand_16</th>\n",
       "      <th>z_right_hand_17</th>\n",
       "      <th>z_right_hand_18</th>\n",
       "      <th>z_right_hand_19</th>\n",
       "      <th>z_right_hand_20</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sequence_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1494438637</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.558105</td>\n",
       "      <td>0.550781</td>\n",
       "      <td>0.554199</td>\n",
       "      <td>0.543945</td>\n",
       "      <td>0.551270</td>\n",
       "      <td>0.555176</td>\n",
       "      <td>0.566406</td>\n",
       "      <td>0.455322</td>\n",
       "      <td>0.572266</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.124939</td>\n",
       "      <td>-0.113708</td>\n",
       "      <td>-0.103394</td>\n",
       "      <td>-0.118713</td>\n",
       "      <td>-0.091064</td>\n",
       "      <td>-0.071289</td>\n",
       "      <td>-0.095154</td>\n",
       "      <td>-0.091675</td>\n",
       "      <td>-0.064819</td>\n",
       "      <td>-0.046692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1494438637</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.559082</td>\n",
       "      <td>0.550781</td>\n",
       "      <td>0.554199</td>\n",
       "      <td>0.543457</td>\n",
       "      <td>0.551270</td>\n",
       "      <td>0.554688</td>\n",
       "      <td>0.565430</td>\n",
       "      <td>0.455566</td>\n",
       "      <td>0.571289</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.101074</td>\n",
       "      <td>-0.092773</td>\n",
       "      <td>-0.083496</td>\n",
       "      <td>-0.102844</td>\n",
       "      <td>-0.077515</td>\n",
       "      <td>-0.058441</td>\n",
       "      <td>-0.076904</td>\n",
       "      <td>-0.079407</td>\n",
       "      <td>-0.056061</td>\n",
       "      <td>-0.039825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1494438637</th>\n",
       "      <td>2.0</td>\n",
       "      <td>0.555176</td>\n",
       "      <td>0.551270</td>\n",
       "      <td>0.554688</td>\n",
       "      <td>0.543457</td>\n",
       "      <td>0.551758</td>\n",
       "      <td>0.555176</td>\n",
       "      <td>0.565430</td>\n",
       "      <td>0.454346</td>\n",
       "      <td>0.570801</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.139404</td>\n",
       "      <td>-0.124512</td>\n",
       "      <td>-0.095825</td>\n",
       "      <td>-0.131958</td>\n",
       "      <td>-0.104431</td>\n",
       "      <td>-0.075500</td>\n",
       "      <td>-0.096985</td>\n",
       "      <td>-0.116089</td>\n",
       "      <td>-0.090149</td>\n",
       "      <td>-0.064758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1494438637</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.557129</td>\n",
       "      <td>0.550293</td>\n",
       "      <td>0.553711</td>\n",
       "      <td>0.542969</td>\n",
       "      <td>0.550781</td>\n",
       "      <td>0.554199</td>\n",
       "      <td>0.564453</td>\n",
       "      <td>0.454834</td>\n",
       "      <td>0.570312</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.187744</td>\n",
       "      <td>-0.179443</td>\n",
       "      <td>-0.133179</td>\n",
       "      <td>-0.173218</td>\n",
       "      <td>-0.143188</td>\n",
       "      <td>-0.115540</td>\n",
       "      <td>-0.135498</td>\n",
       "      <td>-0.150513</td>\n",
       "      <td>-0.123901</td>\n",
       "      <td>-0.102112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1494438637</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.557617</td>\n",
       "      <td>0.551270</td>\n",
       "      <td>0.554688</td>\n",
       "      <td>0.543945</td>\n",
       "      <td>0.552246</td>\n",
       "      <td>0.555664</td>\n",
       "      <td>0.565918</td>\n",
       "      <td>0.454590</td>\n",
       "      <td>0.571777</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.167480</td>\n",
       "      <td>-0.152832</td>\n",
       "      <td>-0.127319</td>\n",
       "      <td>-0.171875</td>\n",
       "      <td>-0.133301</td>\n",
       "      <td>-0.097473</td>\n",
       "      <td>-0.145142</td>\n",
       "      <td>-0.160400</td>\n",
       "      <td>-0.122375</td>\n",
       "      <td>-0.090515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1526914806</th>\n",
       "      <td>21.0</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.602051</td>\n",
       "      <td>0.613281</td>\n",
       "      <td>0.584473</td>\n",
       "      <td>0.598633</td>\n",
       "      <td>0.598633</td>\n",
       "      <td>0.602051</td>\n",
       "      <td>0.485596</td>\n",
       "      <td>0.598633</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.346924</td>\n",
       "      <td>-0.397217</td>\n",
       "      <td>-0.216919</td>\n",
       "      <td>-0.324951</td>\n",
       "      <td>-0.403564</td>\n",
       "      <td>-0.456055</td>\n",
       "      <td>-0.235718</td>\n",
       "      <td>-0.330322</td>\n",
       "      <td>-0.371826</td>\n",
       "      <td>-0.400146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1526914806</th>\n",
       "      <td>22.0</td>\n",
       "      <td>0.625488</td>\n",
       "      <td>0.600586</td>\n",
       "      <td>0.612305</td>\n",
       "      <td>0.583496</td>\n",
       "      <td>0.597168</td>\n",
       "      <td>0.597656</td>\n",
       "      <td>0.600586</td>\n",
       "      <td>0.484619</td>\n",
       "      <td>0.597656</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.189209</td>\n",
       "      <td>-0.222778</td>\n",
       "      <td>-0.099060</td>\n",
       "      <td>-0.173584</td>\n",
       "      <td>-0.230835</td>\n",
       "      <td>-0.266113</td>\n",
       "      <td>-0.110291</td>\n",
       "      <td>-0.190186</td>\n",
       "      <td>-0.219727</td>\n",
       "      <td>-0.233276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1526914806</th>\n",
       "      <td>23.0</td>\n",
       "      <td>0.623047</td>\n",
       "      <td>0.602051</td>\n",
       "      <td>0.613770</td>\n",
       "      <td>0.584473</td>\n",
       "      <td>0.598633</td>\n",
       "      <td>0.598633</td>\n",
       "      <td>0.601562</td>\n",
       "      <td>0.485352</td>\n",
       "      <td>0.598145</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1526914806</th>\n",
       "      <td>24.0</td>\n",
       "      <td>0.623047</td>\n",
       "      <td>0.598633</td>\n",
       "      <td>0.610840</td>\n",
       "      <td>0.582520</td>\n",
       "      <td>0.595215</td>\n",
       "      <td>0.596191</td>\n",
       "      <td>0.601074</td>\n",
       "      <td>0.484863</td>\n",
       "      <td>0.598145</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.343750</td>\n",
       "      <td>-0.400146</td>\n",
       "      <td>-0.210571</td>\n",
       "      <td>-0.305420</td>\n",
       "      <td>-0.387207</td>\n",
       "      <td>-0.445312</td>\n",
       "      <td>-0.230957</td>\n",
       "      <td>-0.315918</td>\n",
       "      <td>-0.359375</td>\n",
       "      <td>-0.391602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1526914806</th>\n",
       "      <td>25.0</td>\n",
       "      <td>0.625488</td>\n",
       "      <td>0.598145</td>\n",
       "      <td>0.608887</td>\n",
       "      <td>0.582031</td>\n",
       "      <td>0.595215</td>\n",
       "      <td>0.596191</td>\n",
       "      <td>0.600586</td>\n",
       "      <td>0.479248</td>\n",
       "      <td>0.598633</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.404541</td>\n",
       "      <td>-0.463623</td>\n",
       "      <td>-0.246948</td>\n",
       "      <td>-0.366211</td>\n",
       "      <td>-0.449463</td>\n",
       "      <td>-0.507812</td>\n",
       "      <td>-0.266602</td>\n",
       "      <td>-0.373535</td>\n",
       "      <td>-0.434326</td>\n",
       "      <td>-0.482178</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>165396 rows × 1630 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             frame  x_face_0  x_face_1  x_face_2  x_face_3  x_face_4  \\\n",
       "sequence_id                                                            \n",
       "1494438637     0.0  0.558105  0.550781  0.554199  0.543945  0.551270   \n",
       "1494438637     1.0  0.559082  0.550781  0.554199  0.543457  0.551270   \n",
       "1494438637     2.0  0.555176  0.551270  0.554688  0.543457  0.551758   \n",
       "1494438637     3.0  0.557129  0.550293  0.553711  0.542969  0.550781   \n",
       "1494438637     4.0  0.557617  0.551270  0.554688  0.543945  0.552246   \n",
       "...            ...       ...       ...       ...       ...       ...   \n",
       "1526914806    21.0  0.625000  0.602051  0.613281  0.584473  0.598633   \n",
       "1526914806    22.0  0.625488  0.600586  0.612305  0.583496  0.597168   \n",
       "1526914806    23.0  0.623047  0.602051  0.613770  0.584473  0.598633   \n",
       "1526914806    24.0  0.623047  0.598633  0.610840  0.582520  0.595215   \n",
       "1526914806    25.0  0.625488  0.598145  0.608887  0.582031  0.595215   \n",
       "\n",
       "             x_face_5  x_face_6  x_face_7  x_face_8  ...  z_right_hand_11  \\\n",
       "sequence_id                                          ...                    \n",
       "1494438637   0.555176  0.566406  0.455322  0.572266  ...        -0.124939   \n",
       "1494438637   0.554688  0.565430  0.455566  0.571289  ...        -0.101074   \n",
       "1494438637   0.555176  0.565430  0.454346  0.570801  ...        -0.139404   \n",
       "1494438637   0.554199  0.564453  0.454834  0.570312  ...        -0.187744   \n",
       "1494438637   0.555664  0.565918  0.454590  0.571777  ...        -0.167480   \n",
       "...               ...       ...       ...       ...  ...              ...   \n",
       "1526914806   0.598633  0.602051  0.485596  0.598633  ...        -0.346924   \n",
       "1526914806   0.597656  0.600586  0.484619  0.597656  ...        -0.189209   \n",
       "1526914806   0.598633  0.601562  0.485352  0.598145  ...              NaN   \n",
       "1526914806   0.596191  0.601074  0.484863  0.598145  ...        -0.343750   \n",
       "1526914806   0.596191  0.600586  0.479248  0.598633  ...        -0.404541   \n",
       "\n",
       "             z_right_hand_12  z_right_hand_13  z_right_hand_14  \\\n",
       "sequence_id                                                      \n",
       "1494438637         -0.113708        -0.103394        -0.118713   \n",
       "1494438637         -0.092773        -0.083496        -0.102844   \n",
       "1494438637         -0.124512        -0.095825        -0.131958   \n",
       "1494438637         -0.179443        -0.133179        -0.173218   \n",
       "1494438637         -0.152832        -0.127319        -0.171875   \n",
       "...                      ...              ...              ...   \n",
       "1526914806         -0.397217        -0.216919        -0.324951   \n",
       "1526914806         -0.222778        -0.099060        -0.173584   \n",
       "1526914806               NaN              NaN              NaN   \n",
       "1526914806         -0.400146        -0.210571        -0.305420   \n",
       "1526914806         -0.463623        -0.246948        -0.366211   \n",
       "\n",
       "             z_right_hand_15  z_right_hand_16  z_right_hand_17  \\\n",
       "sequence_id                                                      \n",
       "1494438637         -0.091064        -0.071289        -0.095154   \n",
       "1494438637         -0.077515        -0.058441        -0.076904   \n",
       "1494438637         -0.104431        -0.075500        -0.096985   \n",
       "1494438637         -0.143188        -0.115540        -0.135498   \n",
       "1494438637         -0.133301        -0.097473        -0.145142   \n",
       "...                      ...              ...              ...   \n",
       "1526914806         -0.403564        -0.456055        -0.235718   \n",
       "1526914806         -0.230835        -0.266113        -0.110291   \n",
       "1526914806               NaN              NaN              NaN   \n",
       "1526914806         -0.387207        -0.445312        -0.230957   \n",
       "1526914806         -0.449463        -0.507812        -0.266602   \n",
       "\n",
       "             z_right_hand_18  z_right_hand_19  z_right_hand_20  \n",
       "sequence_id                                                     \n",
       "1494438637         -0.091675        -0.064819        -0.046692  \n",
       "1494438637         -0.079407        -0.056061        -0.039825  \n",
       "1494438637         -0.116089        -0.090149        -0.064758  \n",
       "1494438637         -0.150513        -0.123901        -0.102112  \n",
       "1494438637         -0.160400        -0.122375        -0.090515  \n",
       "...                      ...              ...              ...  \n",
       "1526914806         -0.330322        -0.371826        -0.400146  \n",
       "1526914806         -0.190186        -0.219727        -0.233276  \n",
       "1526914806               NaN              NaN              NaN  \n",
       "1526914806         -0.315918        -0.359375        -0.391602  \n",
       "1526914806         -0.373535        -0.434326        -0.482178  \n",
       "\n",
       "[165396 rows x 1630 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Shape of the dataset: {df.shape}\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "23ba2f45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gladys guzman',\n",
       " '8780 old 13th',\n",
       " '+62-879-37-82',\n",
       " 'www.china-ycyb.com/arduinoprof',\n",
       " '7642 south beulah road']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "par_path = Path(\"/Users/arvinprince/pytorch-files/Google ASL/data/296317215.parquet\")\n",
    "phrases = train_csv.groupby(\"file_id\").get_group(int(par_path.stem)\n",
    "                                      ).sort_values(by=\"sequence_id\").phrase.tolist()\n",
    "phrases[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "5ba830bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def character_indexer(string:str):\n",
    "    return np.array(list(map(lambda x: CHAR2ORD[x] +2, list(string))), dtype=np.uint16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9d4651c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.layers.TextVectorization(max_tokens=61,\n",
    "                                              split=\"character\",\n",
    "                                              standardize=\"lower\",\n",
    "                                              output_mode=\"int\",\n",
    "#                                               pad_to_max_tokens=True,\n",
    "                                              vocabulary=tuple(CHAR2ORD.keys()),\n",
    "                                              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "908a92b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(31,), dtype=int64, numpy=\n",
       "array([40, 45, 34, 37, 58, 52,  2, 40, 54, 59, 46, 34, 47,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer(phrases[0]) == character_indexer(string=phrases[0])\n",
    "tokenizer(phrases)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a5c2cd5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " '!': 1,\n",
       " '#': 2,\n",
       " '$': 3,\n",
       " '%': 4,\n",
       " '&': 5,\n",
       " \"'\": 6,\n",
       " '(': 7,\n",
       " ')': 8,\n",
       " '*': 9,\n",
       " '+': 10,\n",
       " ',': 11,\n",
       " '-': 12,\n",
       " '.': 13,\n",
       " '/': 14,\n",
       " '0': 15,\n",
       " '1': 16,\n",
       " '2': 17,\n",
       " '3': 18,\n",
       " '4': 19,\n",
       " '5': 20,\n",
       " '6': 21,\n",
       " '7': 22,\n",
       " '8': 23,\n",
       " '9': 24,\n",
       " ':': 25,\n",
       " ';': 26,\n",
       " '=': 27,\n",
       " '?': 28,\n",
       " '@': 29,\n",
       " '[': 30,\n",
       " '_': 31,\n",
       " 'a': 32,\n",
       " 'b': 33,\n",
       " 'c': 34,\n",
       " 'd': 35,\n",
       " 'e': 36,\n",
       " 'f': 37,\n",
       " 'g': 38,\n",
       " 'h': 39,\n",
       " 'i': 40,\n",
       " 'j': 41,\n",
       " 'k': 42,\n",
       " 'l': 43,\n",
       " 'm': 44,\n",
       " 'n': 45,\n",
       " 'o': 46,\n",
       " 'p': 47,\n",
       " 'q': 48,\n",
       " 'r': 49,\n",
       " 's': 50,\n",
       " 't': 51,\n",
       " 'u': 52,\n",
       " 'v': 53,\n",
       " 'w': 54,\n",
       " 'x': 55,\n",
       " 'y': 56,\n",
       " 'z': 57,\n",
       " '~': 58}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CHAR2ORD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a9d150",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"pizza\" \"burger\" \"steak\"\n",
    "0.35 0.33 .31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b89e9866",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " '[UNK]',\n",
       " ' ',\n",
       " '!',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " ';',\n",
       " '=',\n",
       " '?',\n",
       " '@',\n",
       " '[',\n",
       " '_',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'q',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " '~']"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.get_vocabulary()\n",
    "# len(tuple(CHAR2ORD.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff6c41e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = df.fillna(0).groupby(\"sequence_id\").get_group(1526914806\n",
    "#                                                  ).diff(axis=0).dropna().values[:,1:546]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e190939b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "19b12bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ASLFR_Dataset(data.Dataset):\n",
    "    def __init__(self,\n",
    "                 landmark_p: Path,\n",
    "                 landmark_df: pd.DataFrame,\n",
    "                 phrase: pd.DataFrame):\n",
    "        self.l = landmark_p\n",
    "        self.df = landmark_df.astype(np.float32).reset_index()\n",
    "        \n",
    "        self.df.fillna(0, inplace=True)\n",
    "        self.df.drop([col for col in self.df.columns if \"z\" in col], axis = 1, inplace=True)\n",
    "        self.df.drop([col for col in self.df.columns if \"pose\" in col and int(col[7:]) >= 25], \n",
    "                axis = 1, inplace=True)\n",
    "        self.df.drop(\"frame\", axis = 1, inplace=True)\n",
    "        \n",
    "        self.p = train_csv.groupby(\"file_id\").get_group(int(self.l.stem)\n",
    "                                                       ).sort_values(by=\"sequence_id\").phrase.values\n",
    "    \n",
    "#     def load_dataset(self):\n",
    "#         return pd.read_parquet(self.l).astype(np.float32).reset_index()\n",
    "    \n",
    "#     def filtering_coordinates(self,\n",
    "#                             df:pd.DataFrame):\n",
    "#         df.fillna(0, inplace=True)\n",
    "#         df.drop([col for col in df.columns if \"z\" in col], axis = 1, inplace=True)\n",
    "#         df.drop([col for col in df.columns if \"pose\" in col and int(col[7:]) >= 25], axis = 1, inplace=True)\n",
    "# #         df.drop([y for y in df.columns if \"y\" in y], inplace= True, axis = 1)\n",
    "#         df.drop([\"frame\"], axis=1, inplace=True)\n",
    "#         return df\n",
    "    \n",
    "    def character_indexer(self,string:str):\n",
    "        return np.array(list(map(lambda x: CHAR2ORD[x], list(string))), dtype=np.uint32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.p)\n",
    "    \n",
    "    def __getitem__(self, \n",
    "                    index:int):\n",
    "        s_id = self.df.sequence_id[index]\n",
    "        phrase = self.p[index]\n",
    "#         tensor = smart_resize(\n",
    "        x_val = self.df.groupby(\"sequence_id\"\n",
    "                               ).get_group(s_id).diff().dropna().values[:,1:536]\n",
    "        y_val = self.df.groupby(\"sequence_id\"\n",
    "                               ).get_group(s_id).diff().dropna().values[:,536:]\n",
    "#         tensor = np.dstack((self.df.groupby(\"sequence_id\").get_group(s_id).values[:,1:536],\n",
    "#                        self.df.groupby(\"sequence_id\").get_group(s_id).values[:,536:])),\n",
    "        tensor = (np.sqrt(np.square(x_val)+np.square(y_val)))\n",
    "#             (256,256)\n",
    "#         )\n",
    "        return (tensor.flatten(), self.character_indexer(phrase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e644ed53",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = ASLFR_Dataset(landmark_p=Path(\"/Users/arvinprince/pytorch-files/\"\n",
    "                              \"Google ASL/data/296317215.parquet\"),\n",
    "              landmark_df=df,\n",
    "              phrase=train_csv\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0e058a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/38/jtldm9cn7qjfn516_22dcgcm0000gn/T/ipykernel_33153/2328165485.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  np.save(\"X.npy\", np.array(train_ds))\n"
     ]
    }
   ],
   "source": [
    "np.save(\"X.npy\", np.array(train_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c47bd8",
   "metadata": {},
   "source": [
    "here are the things to be done if you forget <br>\n",
    "create a training dataset by taking out the coords and then mathcing them with the phrases and then passing them through a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3b4e21ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = np.load(\"X.npy\", allow_pickle=True).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "fb13430f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = y[0]\n",
    "input_sequence=[]\n",
    "for i in range(1, len(sequence)):\n",
    "    input_sequence.append(sequence[:i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "73241432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([38, 43], dtype=uint32),\n",
       " array([38, 43, 32], dtype=uint32),\n",
       " array([38, 43, 32, 35], dtype=uint32),\n",
       " array([38, 43, 32, 35, 56], dtype=uint32),\n",
       " array([38, 43, 32, 35, 56, 50], dtype=uint32),\n",
       " array([38, 43, 32, 35, 56, 50,  0], dtype=uint32),\n",
       " array([38, 43, 32, 35, 56, 50,  0, 38], dtype=uint32),\n",
       " array([38, 43, 32, 35, 56, 50,  0, 38, 52], dtype=uint32),\n",
       " array([38, 43, 32, 35, 56, 50,  0, 38, 52, 57], dtype=uint32),\n",
       " array([38, 43, 32, 35, 56, 50,  0, 38, 52, 57, 44], dtype=uint32),\n",
       " array([38, 43, 32, 35, 56, 50,  0, 38, 52, 57, 44, 32], dtype=uint32),\n",
       " array([38, 43, 32, 35, 56, 50,  0, 38, 52, 57, 44, 32, 45], dtype=uint32)]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc59feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X= tf.ragged.constant(X.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e998d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "25098579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,)"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7dc842bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "# model.add(keras.layers.BatchNormalization())\n",
    "model.add(keras.layers.Conv2D(64, (5,5)))\n",
    "model.add(keras.layers.MaxPool2D((2,2)))\n",
    "model.add(keras.layers.Conv2D(64, (3,3)))\n",
    "model.add(keras.layers.MaxPool2D((2,2)))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(256, activation=\"relu\"))\n",
    "model.add(keras.layers.Dense(64, activation=\"tanh\"))\n",
    "# model.add(keras.layers.Embedding(64,240))\n",
    "# model.add(keras.layers.Bidirectional(keras.layers.LSTM(150)))\n",
    "model.add(keras.layers.Dense(31, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7b8aedd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "             optimizer= keras.optimizers.legacy.Adam(learning_rate=0.01), \n",
    "             metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a76f3025",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/38/jtldm9cn7qjfn516_22dcgcm0000gn/T/ipykernel_35548/1485580575.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniforge3/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray)."
     ]
    }
   ],
   "source": [
    "model.fit(X, y, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "b278f6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 38ms/step\n"
     ]
    }
   ],
   "source": [
    "l = model.predict(X[0].reshape((1,256,256,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "4fbb3e11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.3485312e-26, 7.9447412e-12, 8.3316898e-08, 3.0691541e-07,\n",
       "        6.4937849e-06, 2.4789362e-05, 3.8392318e-05, 3.5051026e-04,\n",
       "        5.9117848e-04, 1.6003705e-03, 1.5310565e-03, 7.9629786e-04,\n",
       "        1.3677583e-02, 3.4777613e-03, 3.9988495e-03, 6.1395294e-03,\n",
       "        4.8061758e-03, 5.0305882e-03, 2.4779378e-02, 3.8109012e-02,\n",
       "        5.1720563e-02, 5.7593562e-02, 8.3091762e-03, 1.5193112e-01,\n",
       "        9.8334633e-02, 7.5005894e-03, 2.3799855e-01, 9.8285884e-02,\n",
       "        9.1578579e-03, 3.8220968e-02, 1.3598874e-01]], dtype=float32)"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d7de4f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
